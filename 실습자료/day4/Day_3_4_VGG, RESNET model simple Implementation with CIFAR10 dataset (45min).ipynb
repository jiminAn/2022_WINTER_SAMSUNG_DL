{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1UpQVm6iH6RDOKjU-fl5gTfDsc80fOOxZ","timestamp":1627397450310}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"52a2f7e3ba2c4e1daa0448e975ecd7ac":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f5be5d6d52fc4d83b1aa87106b9f8b92","IPY_MODEL_a75f21939a8742f58b392d85b49e5775","IPY_MODEL_8361b05438314275bd8428332578a529"],"layout":"IPY_MODEL_02a02900efd244aa8ac8ca6876167d20"}},"f5be5d6d52fc4d83b1aa87106b9f8b92":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f6f795335d34822851ed293b667402b","placeholder":"​","style":"IPY_MODEL_f44693111c0b4414b7bdd0672c65ce26","value":"100%"}},"a75f21939a8742f58b392d85b49e5775":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_257c85b0d03344dc9cddd9c53ac514fc","max":170498071,"min":0,"orientation":"horizontal","style":"IPY_MODEL_913f66adb09c41d2aaac2662dc9f7a09","value":170498071}},"8361b05438314275bd8428332578a529":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c1f71ddbff543c5b51c10943084ea02","placeholder":"​","style":"IPY_MODEL_e513b388a99f4802b0d93199b4af3fe0","value":" 170498071/170498071 [00:14&lt;00:00, 14680502.40it/s]"}},"02a02900efd244aa8ac8ca6876167d20":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f6f795335d34822851ed293b667402b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f44693111c0b4414b7bdd0672c65ce26":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"257c85b0d03344dc9cddd9c53ac514fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"913f66adb09c41d2aaac2662dc9f7a09":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8c1f71ddbff543c5b51c10943084ea02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e513b388a99f4802b0d93199b4af3fe0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"D9UUcbMe5uHd"},"source":["# - LAB4 : VGG, RESNET model simple Implementation with CIFAR10 dataset (45 min)\n","#### LAB4 에서는 CNN 아키텍쳐 중 가장 대표적인 VGG과 RESNET 모델을 간단히 구현해보고 CIFAR-10 데이터셋에 대해 학습 및 Inference를 해볼 계획입니다."]},{"cell_type":"markdown","metadata":{"id":"pMY3CPV4ebV2"},"source":["## Example 1) VGG16 모델 구현\n","- [doc] (https://arxiv.org/pdf/1409.1556.pdf)\n","\n","![image.png](http://drive.google.com/uc?id=1E6MVIcFCsImwWQGOhC-8KUQqAe2W_I28)\n","\n","![image.png](http://drive.google.com/uc?id=1jT9jhqMzEaHoma5xvro5jf6PFOq7FlLJ)\n","\n","![image.png](http://drive.google.com/uc?id=17DgT11woHwXACEGOfvkVd8pYmxRI7vQl)"]},{"cell_type":"code","metadata":{"id":"jbce4wfMMTqv","colab":{"base_uri":"https://localhost:8080/","height":101,"referenced_widgets":["52a2f7e3ba2c4e1daa0448e975ecd7ac","f5be5d6d52fc4d83b1aa87106b9f8b92","a75f21939a8742f58b392d85b49e5775","8361b05438314275bd8428332578a529","02a02900efd244aa8ac8ca6876167d20","1f6f795335d34822851ed293b667402b","f44693111c0b4414b7bdd0672c65ce26","257c85b0d03344dc9cddd9c53ac514fc","913f66adb09c41d2aaac2662dc9f7a09","8c1f71ddbff543c5b51c10943084ea02","e513b388a99f4802b0d93199b4af3fe0"]},"executionInfo":{"status":"ok","timestamp":1673501855414,"user_tz":-540,"elapsed":24369,"user":{"displayName":"인공지능학과/최준수","userId":"03698814127107115075"}},"outputId":"37a5da28-68d5-4a1f-c4f2-c8f2f9ee275c"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torchvision import transforms, datasets, utils\n","from torchsummary import summary\n","import matplotlib.pyplot as plt\n","\n","# DEVICE 설정\n","USE_CUDA = torch.cuda.is_available()\n","DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n","\n","# Parameter 설정\n","EPOCHS = 10\n","BATCH_SIZE = 64\n","LR = 0.0001\n","\n","# Transform 설정\n","transform_CIFAR10 = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # ImageNet 데이터셋의 평균과 분산\n","])\n","\n","# Dataset 설정\n","train_dataset = datasets.CIFAR10(root = '../data',\n","                                         train = True,\n","                                         download = True,\n","                                         transform = transform_CIFAR10)\n","\n","test_dataset = datasets.CIFAR10(root = '../data',\n","                                train = False,\n","                                download = True,\n","                                transform = transform_CIFAR10)\n","\n","train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n","                                           batch_size = BATCH_SIZE,\n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n","                                          batch_size = BATCH_SIZE,\n","                                          shuffle=False)"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data/cifar-10-python.tar.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52a2f7e3ba2c4e1daa0448e975ecd7ac"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ../data/cifar-10-python.tar.gz to ../data\n","Files already downloaded and verified\n"]}]},{"cell_type":"code","metadata":{"id":"_JeDJmfNuN0k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673502911656,"user_tz":-540,"elapsed":616,"user":{"displayName":"인공지능학과/최준수","userId":"03698814127107115075"}},"outputId":"42ca3be6-2391-4212-99b6-234df9246bcc"},"source":["# Model 구현\n","class Custom_VGG(nn.Module):\n","    def __init__(self):\n","        super(Custom_VGG, self).__init__()\n","        ########################################## Complete This Code~!\n","        self.conv1_1=nn.Conv2d(3, 64, 3, padding=1)\n","        self.conv1_2=nn.Conv2d(64, 64, 3, padding=1)\n","\n","        self.conv2_1=nn.Conv2d(64, 128, 3, padding=1)\n","        self.conv2_2=nn.Conv2d(128, 128, 3, padding=1)\n","\n","        self.conv3_1=nn.Conv2d(128, 256, 3, padding=1)\n","        self.conv3_2=nn.Conv2d(256, 256, 3, padding=1)\n","        self.conv3_3=nn.Conv2d(256, 256, 3, padding=1)\n","\n","        self.conv4_1=nn.Conv2d(256, 512, 3, padding=1)\n","        self.conv4_2=nn.Conv2d(512, 512, 3, padding=1)\n","        self.conv4_3=nn.Conv2d(512, 512, 3, padding=1)\n","\n","        self.conv5_1=nn.Conv2d(512, 512, 3, padding=1)\n","        self.conv5_2=nn.Conv2d(512, 512, 3, padding=1)\n","        self.conv5_3=nn.Conv2d(512, 512, 3, padding=1)\n","\n","        self.adaptiveavgpool2d = nn.AdaptiveAvgPool2d((7, 7))\n","\n","        self.fc1 = nn.Linear(512*7*7, 4096)\n","        self.fc2 = nn.Linear(4096, 4096)\n","        self.fc3 = nn.Linear(4096, 10)\n","\n","        self.maxpool2d = nn.MaxPool2d(2)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.dropout = nn.Dropout()\n","\n","        ########################################## Complete This Code~!\n","\n","    def forward(self, x):\n","        ########################################## Complete This Code~!\n","        x = self.relu(self.conv1_1(x))\n","        x = self.relu(self.conv1_2(x))\n","        x = self.maxpool2d(x)\n","\n","        x = self.relu(self.conv2_1(x))\n","        x = self.relu(self.conv2_2(x))\n","        x = self.maxpool2d(x)\n","\n","        x = self.relu(self.conv3_1(x))\n","        x = self.relu(self.conv3_2(x))\n","        x = self.relu(self.conv3_3(x))\n","        x = self.maxpool2d(x)\n","\n","        x = self.relu(self.conv4_1(x))\n","        x = self.relu(self.conv4_2(x))\n","        x = self.relu(self.conv4_3(x))\n","        x = self.maxpool2d(x)\n","\n","        x = self.relu(self.conv5_1(x))\n","        x = self.relu(self.conv5_2(x))\n","        x = self.relu(self.conv5_3(x))\n","        x = self.maxpool2d(x)\n","\n","        x = self.adaptiveavgpool2d(x)\n","        # flatten\n","        x = x.view(-1, 512*7*7)\n","        \n","        x = self.dropout(self.relu(self.fc1(x)))\n","        x = self.dropout(self.relu(self.fc2(x)))\n","        x = self.fc3(x)\n","\n","        ########################################## Complete This Code~!\n","\n","        return x\n","model = Custom_VGG().to(DEVICE)\n","summary(model, (3,32,32))"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 64, 32, 32]           1,792\n","              ReLU-2           [-1, 64, 32, 32]               0\n","            Conv2d-3           [-1, 64, 32, 32]          36,928\n","              ReLU-4           [-1, 64, 32, 32]               0\n","         MaxPool2d-5           [-1, 64, 16, 16]               0\n","            Conv2d-6          [-1, 128, 16, 16]          73,856\n","              ReLU-7          [-1, 128, 16, 16]               0\n","            Conv2d-8          [-1, 128, 16, 16]         147,584\n","              ReLU-9          [-1, 128, 16, 16]               0\n","        MaxPool2d-10            [-1, 128, 8, 8]               0\n","           Conv2d-11            [-1, 256, 8, 8]         295,168\n","             ReLU-12            [-1, 256, 8, 8]               0\n","           Conv2d-13            [-1, 256, 8, 8]         590,080\n","             ReLU-14            [-1, 256, 8, 8]               0\n","           Conv2d-15            [-1, 256, 8, 8]         590,080\n","             ReLU-16            [-1, 256, 8, 8]               0\n","        MaxPool2d-17            [-1, 256, 4, 4]               0\n","           Conv2d-18            [-1, 512, 4, 4]       1,180,160\n","             ReLU-19            [-1, 512, 4, 4]               0\n","           Conv2d-20            [-1, 512, 4, 4]       2,359,808\n","             ReLU-21            [-1, 512, 4, 4]               0\n","           Conv2d-22            [-1, 512, 4, 4]       2,359,808\n","             ReLU-23            [-1, 512, 4, 4]               0\n","        MaxPool2d-24            [-1, 512, 2, 2]               0\n","           Conv2d-25            [-1, 512, 2, 2]       2,359,808\n","             ReLU-26            [-1, 512, 2, 2]               0\n","           Conv2d-27            [-1, 512, 2, 2]       2,359,808\n","             ReLU-28            [-1, 512, 2, 2]               0\n","           Conv2d-29            [-1, 512, 2, 2]       2,359,808\n","             ReLU-30            [-1, 512, 2, 2]               0\n","        MaxPool2d-31            [-1, 512, 1, 1]               0\n","AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n","           Linear-33                 [-1, 4096]     102,764,544\n","             ReLU-34                 [-1, 4096]               0\n","          Dropout-35                 [-1, 4096]               0\n","           Linear-36                 [-1, 4096]      16,781,312\n","             ReLU-37                 [-1, 4096]               0\n","          Dropout-38                 [-1, 4096]               0\n","           Linear-39                   [-1, 10]          40,970\n","================================================================\n","Total params: 134,301,514\n","Trainable params: 134,301,514\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 4.84\n","Params size (MB): 512.32\n","Estimated Total Size (MB): 517.17\n","----------------------------------------------------------------\n"]}]},{"cell_type":"code","metadata":{"id":"jkqHEqDBaoPQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673502953701,"user_tz":-540,"elapsed":3052,"user":{"displayName":"인공지능학과/최준수","userId":"03698814127107115075"}},"outputId":"c0c4b67a-b0df-4dc8-91bb-c2601d347efc"},"source":["from torchvision import models\n","model_import = models.vgg16(pretrained=False, num_classes=10).to(DEVICE)\n","summary(model_import, (3, 32, 32))"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 64, 32, 32]           1,792\n","              ReLU-2           [-1, 64, 32, 32]               0\n","            Conv2d-3           [-1, 64, 32, 32]          36,928\n","              ReLU-4           [-1, 64, 32, 32]               0\n","         MaxPool2d-5           [-1, 64, 16, 16]               0\n","            Conv2d-6          [-1, 128, 16, 16]          73,856\n","              ReLU-7          [-1, 128, 16, 16]               0\n","            Conv2d-8          [-1, 128, 16, 16]         147,584\n","              ReLU-9          [-1, 128, 16, 16]               0\n","        MaxPool2d-10            [-1, 128, 8, 8]               0\n","           Conv2d-11            [-1, 256, 8, 8]         295,168\n","             ReLU-12            [-1, 256, 8, 8]               0\n","           Conv2d-13            [-1, 256, 8, 8]         590,080\n","             ReLU-14            [-1, 256, 8, 8]               0\n","           Conv2d-15            [-1, 256, 8, 8]         590,080\n","             ReLU-16            [-1, 256, 8, 8]               0\n","        MaxPool2d-17            [-1, 256, 4, 4]               0\n","           Conv2d-18            [-1, 512, 4, 4]       1,180,160\n","             ReLU-19            [-1, 512, 4, 4]               0\n","           Conv2d-20            [-1, 512, 4, 4]       2,359,808\n","             ReLU-21            [-1, 512, 4, 4]               0\n","           Conv2d-22            [-1, 512, 4, 4]       2,359,808\n","             ReLU-23            [-1, 512, 4, 4]               0\n","        MaxPool2d-24            [-1, 512, 2, 2]               0\n","           Conv2d-25            [-1, 512, 2, 2]       2,359,808\n","             ReLU-26            [-1, 512, 2, 2]               0\n","           Conv2d-27            [-1, 512, 2, 2]       2,359,808\n","             ReLU-28            [-1, 512, 2, 2]               0\n","           Conv2d-29            [-1, 512, 2, 2]       2,359,808\n","             ReLU-30            [-1, 512, 2, 2]               0\n","        MaxPool2d-31            [-1, 512, 1, 1]               0\n","AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n","           Linear-33                 [-1, 4096]     102,764,544\n","             ReLU-34                 [-1, 4096]               0\n","          Dropout-35                 [-1, 4096]               0\n","           Linear-36                 [-1, 4096]      16,781,312\n","             ReLU-37                 [-1, 4096]               0\n","          Dropout-38                 [-1, 4096]               0\n","           Linear-39                   [-1, 10]          40,970\n","================================================================\n","Total params: 134,301,514\n","Trainable params: 134,301,514\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 4.84\n","Params size (MB): 512.32\n","Estimated Total Size (MB): 517.17\n","----------------------------------------------------------------\n"]}]},{"cell_type":"code","metadata":{"id":"_b5IsHnKUquV","executionInfo":{"status":"ok","timestamp":1673502968839,"user_tz":-540,"elapsed":4,"user":{"displayName":"인공지능학과/최준수","userId":"03698814127107115075"}}},"source":["# Optimizer 설정\n","optimizer = optim.Adam(model.parameters(), lr=LR)\n","# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"oy8oXE_mWARx","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b6573dcd-c6fa-4690-a1ae-c70ee287cb68"},"source":["# Train 구현\n","def train_one_epoch(model, train_loader, optimizer, epoch):\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(DEVICE), target.to(DEVICE)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = F.cross_entropy(output, target)\n","        loss.backward()\n","        optimizer.step()\n","\n","        if batch_idx % 200 == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.item()))\n","            \n","# Evaluation 구현\n","def evaluate(model, test_loader):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(DEVICE), target.to(DEVICE)\n","            output = model(data)\n","\n","            # 배치 오차를 합산\n","            test_loss += F.cross_entropy(output, target,\n","                                         reduction='sum').item()\n","\n","            # 가장 높은 값을 가진 인덱스가 바로 예측값\n","            pred = output.max(1, keepdim=True)[1]\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","    test_accuracy = 100. * correct / len(test_loader.dataset)\n","    return test_loss, test_accuracy\n","\n","for epoch in range(1, EPOCHS + 1):\n","    train_one_epoch(model, train_loader, optimizer, epoch)\n","    test_loss, test_accuracy = evaluate(model, test_loader)\n","    \n","    print('[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%'.format(\n","          epoch, test_loss, test_accuracy))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.302446\n","Train Epoch: 1 [12800/50000 (26%)]\tLoss: 1.907495\n","Train Epoch: 1 [25600/50000 (51%)]\tLoss: 1.964128\n","Train Epoch: 1 [38400/50000 (77%)]\tLoss: 2.006992\n","[1] Test Loss: 1.7421, Accuracy: 32.67%\n","Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.784603\n","Train Epoch: 2 [12800/50000 (26%)]\tLoss: 1.787778\n","Train Epoch: 2 [25600/50000 (51%)]\tLoss: 1.662158\n","Train Epoch: 2 [38400/50000 (77%)]\tLoss: 1.497230\n","[2] Test Loss: 1.3843, Accuracy: 48.13%\n","Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.350017\n","Train Epoch: 3 [12800/50000 (26%)]\tLoss: 1.427957\n","Train Epoch: 3 [25600/50000 (51%)]\tLoss: 1.254034\n","Train Epoch: 3 [38400/50000 (77%)]\tLoss: 1.220152\n","[3] Test Loss: 1.1890, Accuracy: 56.21%\n","Train Epoch: 4 [0/50000 (0%)]\tLoss: 1.386639\n","Train Epoch: 4 [12800/50000 (26%)]\tLoss: 1.109573\n","Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.866467\n","Train Epoch: 4 [38400/50000 (77%)]\tLoss: 1.033588\n","[4] Test Loss: 0.9789, Accuracy: 64.56%\n","Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.901854\n","Train Epoch: 5 [12800/50000 (26%)]\tLoss: 1.010237\n","Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.868940\n","Train Epoch: 5 [38400/50000 (77%)]\tLoss: 0.992379\n","[5] Test Loss: 0.8699, Accuracy: 69.75%\n","Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.989167\n","Train Epoch: 6 [12800/50000 (26%)]\tLoss: 0.836403\n"]}]},{"cell_type":"markdown","metadata":{"id":"JNxz2ARrE9Jp"},"source":["## Exercise 1) RESNET18 모델 구현\n","- [doc] (https://arxiv.org/pdf/1512.03385.pdf)\n","\n","![image.png](http://drive.google.com/uc?id=1GgHATI5PFF8-PlBdGp9vDra2maRqLybl)\n","\n","![image.png](http://drive.google.com/uc?id=1EYxIKJEI0rwIyW7ZZaFeZgJvol6Jb-XL)\n","\n","![image.png](http://drive.google.com/uc?id=17DgT11woHwXACEGOfvkVd8pYmxRI7vQl)\n","\n","![image.png](http://drive.google.com/uc?id=12reHf9xtapZrVBG4LlNbNGa37ZeFfUqk)"]},{"cell_type":"code","metadata":{"id":"5_IJRFtgHw02"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torchvision import transforms, datasets, utils\n","from torchsummary import summary\n","import matplotlib.pyplot as plt\n","\n","# DEVICE 설정\n","USE_CUDA = torch.cuda.is_available()\n","DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n","\n","# Parameter 설정\n","EPOCHS = 10\n","BATCH_SIZE = 64\n","LR = 0.01\n","\n","# Transform 설정\n","transform_CIFAR10 = transforms.Compose([\n","    # transforms.Resize((224,224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n","])\n","\n","# Dataset 설정\n","train_dataset = datasets.CIFAR10(root = '../data',\n","                                         train = True,\n","                                         download = True,\n","                                         transform = transform_CIFAR10)\n","\n","test_dataset = datasets.CIFAR10(root = '../data',\n","                                train = False,\n","                                download = True,\n","                                transform = transform_CIFAR10)\n","\n","train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n","                                           batch_size = BATCH_SIZE,\n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n","                                          batch_size = BATCH_SIZE,\n","                                          shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1dykk5Om-cPG"},"source":["# Model 구현\n","class Custom_RESNET(nn.Module):\n","    def __init__(self):\n","        super(Custom_RESNET, self).__init__()\n","        self.maxpool2d = nn.MaxPool2d(kernel_size=3, stride=2)\n","        self.relu = nn.ReLU(inplace=True)\n","\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=1)\n","        self.bn1 = nn.BatchNorm2d(num_features=64)\n","        \n","        self.conv2_1 = nn.Conv2d(64,64,3,padding=1)\n","        self.conv2_2 = nn.Conv2d(64,64,3,padding=1)\n","        self.conv2_3 = nn.Conv2d(64,64,3,padding=1)\n","        self.conv2_4 = nn.Conv2d(64,64,3,padding=1)\n","        self.bn2_1 = nn.BatchNorm2d(num_features=64)\n","        self.bn2_2 = nn.BatchNorm2d(num_features=64)\n","        self.bn2_3 = nn.BatchNorm2d(num_features=64)\n","        self.bn2_4 = nn.BatchNorm2d(num_features=64)\n","\n","        self.conv3_1 = nn.Conv2d(64,128,3,padding=1, stride=2)\n","        self.conv3_2 = nn.Conv2d(128,128,3,padding=1)\n","        self.conv3_3 = nn.Conv2d(128,128,3,padding=1)\n","        self.conv3_4 = nn.Conv2d(128,128,3,padding=1)\n","        self.bn3_1 = nn.BatchNorm2d(num_features=128)\n","        self.bn3_2 = nn.BatchNorm2d(num_features=128)\n","        self.bn3_3 = nn.BatchNorm2d(num_features=128)\n","        self.bn3_4 = nn.BatchNorm2d(num_features=128)\n","\n","        self.conv4_1 = nn.Conv2d(128,256,3,padding=1, stride=2)\n","        self.conv4_2 = nn.Conv2d(256,256,3,padding=1)\n","        self.conv4_3 = nn.Conv2d(256,256,3,padding=1)\n","        self.conv4_4 = nn.Conv2d(256,256,3,padding=1)\n","        self.bn4_1 = nn.BatchNorm2d(num_features=256)\n","        self.bn4_2 = nn.BatchNorm2d(num_features=256)\n","        self.bn4_3 = nn.BatchNorm2d(num_features=256)\n","        self.bn4_4 = nn.BatchNorm2d(num_features=256)\n","\n","        self.conv5_1 = nn.Conv2d(256,512,3,padding=1, stride=2)\n","        self.conv5_2 = nn.Conv2d(512,512,3,padding=1)\n","        self.conv5_3 = nn.Conv2d(512,512,3,padding=1)\n","        self.conv5_4 = nn.Conv2d(512,512,3,padding=1)\n","        self.bn5_1 = nn.BatchNorm2d(num_features=512)\n","        self.bn5_2 = nn.BatchNorm2d(num_features=512)\n","        self.bn5_3 = nn.BatchNorm2d(num_features=512)\n","        self.bn5_4 = nn.BatchNorm2d(num_features=512)\n","\n","        self.adaptiveavgpool2d = nn.AdaptiveAvgPool2d(1)\n","\n","        self.fc = nn.Linear(512, 10)\n","\n","    def forward(self, x):\n","        ########################################## Complete This Code~!\n","\n","        ########################################## Complete This Code~!\n","\n","        x = self.adaptiveavgpool2d(x5_4)\n","        x = x.view(-1,512)\n","        x = self.fc(x)\n","        return x\n","\n","model = Custom_RESNET().to(DEVICE)\n","summary(model, (3, 32, 32))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YJq3fJZHHw7_"},"source":["from torchvision import models\n","model_import = models.resnet18(pretrained=False, num_classes=10).to(DEVICE)\n","summary(model_import, (3, 32, 32))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pUam0mfLavpg"},"source":["# Model, Optimizer 설정\n","optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GM0Tm7RJHzv-"},"source":["# Train 구현\n","def train_one_epoch(model, train_loader, optimizer, epoch):\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(DEVICE), target.to(DEVICE)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = F.cross_entropy(output, target)\n","        loss.backward()\n","        optimizer.step()\n","\n","        if batch_idx % 200 == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.item()))\n","            \n","# Evaluation 구현\n","def evaluate(model, test_loader):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(DEVICE), target.to(DEVICE)\n","            output = model(data)\n","\n","            # 배치 오차를 합산\n","            test_loss += F.cross_entropy(output, target,\n","                                         reduction='sum').item()\n","\n","            # 가장 높은 값을 가진 인덱스가 바로 예측값\n","            pred = output.max(1, keepdim=True)[1]\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","    test_accuracy = 100. * correct / len(test_loader.dataset)\n","    return test_loss, test_accuracy\n","\n","for epoch in range(1, EPOCHS + 1):\n","    train_one_epoch(model, train_loader, optimizer, epoch)\n","    test_loss, test_accuracy = evaluate(model, test_loader)\n","    \n","    print('[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%'.format(\n","          epoch, test_loss, test_accuracy))"],"execution_count":null,"outputs":[]}]}